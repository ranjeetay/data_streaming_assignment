{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd4eed60-d7fd-4ab2-8ef7-ba201f4ef7f2",
      "metadata": {
        "id": "dd4eed60-d7fd-4ab2-8ef7-ba201f4ef7f2"
      },
      "source": [
        "# Streaming Assginment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700758ef-04d5-44db-96d8-be4b447487b6",
      "metadata": {
        "id": "700758ef-04d5-44db-96d8-be4b447487b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622d047c-cd67-490c-bafc-f164b25a6532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "## Let's first use an API to get the data\n",
        "!pip install pyspark\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import date, timedelta, datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63e0dad2-5675-4609-a5a9-4d11acfee236",
      "metadata": {
        "id": "63e0dad2-5675-4609-a5a9-4d11acfee236"
      },
      "source": [
        "These two functions allow us to create a CSV file (Initialize the file), and then we can update every seconds. Let's start with a streaming: let's populate the CSV file with 1 second of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b9ff90-6cd7-43eb-bb80-c871e667704a",
      "metadata": {
        "id": "02b9ff90-6cd7-43eb-bb80-c871e667704a"
      },
      "outputs": [],
      "source": [
        "## Let's start by setting up the imports and the create the Spark Session\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "app_name = \"Pyspark-sql\"\n",
        "master = \"local[*]\"\n",
        "spark = SparkSession\\\n",
        "        .builder\\\n",
        "        .appName(app_name)\\\n",
        "        .master(master)\\\n",
        "        .config(\"spark.ui.port\",\"42229\")\\\n",
        "        .getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "**Data Description**\n",
        "\n",
        "**Daily_stock_report**\n",
        "- date: daily stock date (datType:TimestampType)\n",
        "- open: daily open stock (datType:FloatType)\n",
        "- high: daily high stock (datType:FloatType)\n",
        "- low: daily low stock (datType:FloatType)\n",
        "- closer: daily closer stock (datType:FloatType)\n",
        "- Adj closer: daily adj stock (datType:FloatType)\n",
        "- volume: daily volumn stock (datType:IntegerType)"
      ],
      "metadata": {
        "id": "_zjyxpmcyrJj"
      },
      "id": "_zjyxpmcyrJj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_streaming = spark.read.csv(\"MSFT.csv\", header=\"true\", inferSchema=\"true\")"
      ],
      "metadata": {
        "id": "QHRBWzZ6LHbx"
      },
      "id": "QHRBWzZ6LHbx",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_events = spark \\\n",
        "    .readStream \\\n",
        "    .format('csv') \\\n",
        "    .schema(raw_schema) \\\n",
        "    .csv('MSFT.csv')"
      ],
      "metadata": {
        "id": "sW8wxUx8n7jc"
      },
      "id": "sW8wxUx8n7jc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "df_streaming.registerTempTable(\"df_streaming\")\n",
        "spark.sql(\"SELECT High,Low,Volume,AVG(High) OVER() AS avg_high FROM df_streaming\").show();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg0RRXlODrPN",
        "outputId": "619126a6-03cb-4397-ef04-9de0ec23212b"
      },
      "id": "tg0RRXlODrPN",
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------+------------------+\n",
            "|     High|      Low|  Volume|          avg_high|\n",
            "+---------+---------+--------+------------------+\n",
            "|64.389999|64.050003|19292700|162.64645748371714|\n",
            "|64.730003|64.190002|20273100|162.64645748371714|\n",
            "|64.800003|64.139999|21796800|162.64645748371714|\n",
            "|64.540001|64.050003|15871500|162.64645748371714|\n",
            "|64.199997|63.759998|23239800|162.64645748371714|\n",
            "|64.989998|64.019997|26937500|162.64645748371714|\n",
            "|    64.75|63.880001|24539600|162.64645748371714|\n",
            "|64.279999|63.619999|18135900|162.64645748371714|\n",
            "|64.559998|63.810001|18750300|162.64645748371714|\n",
            "|64.779999|64.190002|18521000|162.64645748371714|\n",
            "|65.080002|    64.25|21510900|162.64645748371714|\n",
            "|65.199997|64.480003|19846800|162.64645748371714|\n",
            "|65.260002|    64.75|19538200|162.64645748371714|\n",
            "|65.190002|    64.57|20100000|162.64645748371714|\n",
            "|64.550003|64.150002|14280200|162.64645748371714|\n",
            "|64.919998|    64.25|24833800|162.64645748371714|\n",
            "|64.760002|64.300003|20674300|162.64645748371714|\n",
            "|65.239998|    64.68|49219700|162.64645748371714|\n",
            "|    65.18|64.720001|14598100|162.64645748371714|\n",
            "|     65.5|64.129997|26640500|162.64645748371714|\n",
            "+---------+---------+--------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql import functions as F\n",
        "df_streaming = df_streaming.withColumn(\"Weekly_Avg\", F.avg('High').over(Window.partitionBy(\"Date\").orderBy(\"High\").rangeBetween(-7, Window.currentRow)))\n",
        "df_streaming.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-Rcg7tEIU9l",
        "outputId": "166bb5c0-82b3-43a9-e151-e39ff73d6a86"
      },
      "id": "E-Rcg7tEIU9l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+\n",
            "|               Date|     Open|     High|      Low|    Close|Adj Close|  Volume| Avg_high|Weekly_Avg|\n",
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+\n",
            "|2017-02-22 00:00:00|64.330002|64.389999|64.050003|64.360001|60.079075|19292700|64.389999| 64.389999|\n",
            "|2017-02-23 00:00:00|64.419998|64.730003|64.190002|64.620003|60.321793|20273100|64.730003| 64.730003|\n",
            "|2017-02-24 00:00:00|64.529999|64.800003|64.139999|64.620003|60.321793|21796800|64.800003| 64.800003|\n",
            "|2017-02-27 00:00:00|64.540001|64.540001|64.050003|64.230003|59.957733|15871500|64.540001| 64.540001|\n",
            "|2017-02-28 00:00:00|64.080002|64.199997|63.759998|    63.98| 59.72435|23239800|64.199997| 64.199997|\n",
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql import functions as F\n",
        "df_streaming = df_streaming.withColumn(\"Monthly_Avg\", F.avg('High').over(Window.partitionBy(\"Date\").orderBy(\"High\").rangeBetween(-30, Window.currentRow)))\n",
        "df_streaming.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62Pk8zkENW7j",
        "outputId": "5cb0e2e0-eb27-4ed8-9833-1e1d8a8550b4"
      },
      "id": "62Pk8zkENW7j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+-----------+\n",
            "|               Date|     Open|     High|      Low|    Close|Adj Close|  Volume| Avg_high|Weekly_Avg|Monthly_Avg|\n",
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+-----------+\n",
            "|2017-02-22 00:00:00|64.330002|64.389999|64.050003|64.360001|60.079075|19292700|64.389999| 64.389999|  64.389999|\n",
            "|2017-02-23 00:00:00|64.419998|64.730003|64.190002|64.620003|60.321793|20273100|64.730003| 64.730003|  64.730003|\n",
            "|2017-02-24 00:00:00|64.529999|64.800003|64.139999|64.620003|60.321793|21796800|64.800003| 64.800003|  64.800003|\n",
            "|2017-02-27 00:00:00|64.540001|64.540001|64.050003|64.230003|59.957733|15871500|64.540001| 64.540001|  64.540001|\n",
            "|2017-02-28 00:00:00|64.080002|64.199997|63.759998|    63.98| 59.72435|23239800|64.199997| 64.199997|  64.199997|\n",
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql import functions as F\n",
        "df_streaming = df_streaming.withColumn(\"Yearly_Avg\", F.avg('High').over(Window.partitionBy(\"Date\").orderBy(\"High\").rangeBetween(-365, Window.currentRow)))\n",
        "df_streaming.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USYSbarrNwBj",
        "outputId": "a19b867e-5fc4-4685-9ffb-bee8e40d3fc7"
      },
      "id": "USYSbarrNwBj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+-----------+----------+\n",
            "|               Date|     Open|     High|      Low|    Close|Adj Close|  Volume| Avg_high|Weekly_Avg|Monthly_Avg|Yearly_Avg|\n",
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+-----------+----------+\n",
            "|2017-02-22 00:00:00|64.330002|64.389999|64.050003|64.360001|60.079075|19292700|64.389999| 64.389999|  64.389999| 64.389999|\n",
            "|2017-02-23 00:00:00|64.419998|64.730003|64.190002|64.620003|60.321793|20273100|64.730003| 64.730003|  64.730003| 64.730003|\n",
            "|2017-02-24 00:00:00|64.529999|64.800003|64.139999|64.620003|60.321793|21796800|64.800003| 64.800003|  64.800003| 64.800003|\n",
            "|2017-02-27 00:00:00|64.540001|64.540001|64.050003|64.230003|59.957733|15871500|64.540001| 64.540001|  64.540001| 64.540001|\n",
            "|2017-02-28 00:00:00|64.080002|64.199997|63.759998|    63.98| 59.72435|23239800|64.199997| 64.199997|  64.199997| 64.199997|\n",
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql import functions as F\n",
        "df_streaming = df_streaming.withColumn(\"Monthly_Avg\", F.avg('High').over(Window.partitionBy(\"Date\").orderBy(\"High\").rangeBetween(-3, Window.currentRow)))\n",
        "df_streaming.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHoQNDVdN3fp",
        "outputId": "d83821a8-6f7d-4333-8b9e-da18b80b0463"
      },
      "id": "hHoQNDVdN3fp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+-----------+----------+\n",
            "|               Date|     Open|     High|      Low|    Close|Adj Close|  Volume| Avg_high|Weekly_Avg|Monthly_Avg|Yearly_Avg|\n",
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+-----------+----------+\n",
            "|2017-02-22 00:00:00|64.330002|64.389999|64.050003|64.360001|60.079075|19292700|64.389999| 64.389999|  64.389999| 64.389999|\n",
            "|2017-02-23 00:00:00|64.419998|64.730003|64.190002|64.620003|60.321793|20273100|64.730003| 64.730003|  64.730003| 64.730003|\n",
            "|2017-02-24 00:00:00|64.529999|64.800003|64.139999|64.620003|60.321793|21796800|64.800003| 64.800003|  64.800003| 64.800003|\n",
            "|2017-02-27 00:00:00|64.540001|64.540001|64.050003|64.230003|59.957733|15871500|64.540001| 64.540001|  64.540001| 64.540001|\n",
            "|2017-02-28 00:00:00|64.080002|64.199997|63.759998|    63.98| 59.72435|23239800|64.199997| 64.199997|  64.199997| 64.199997|\n",
            "+-------------------+---------+---------+---------+---------+---------+--------+---------+----------+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Validating the missing data in dataset\n",
        "from pyspark.sql import functions as F\n",
        "cols_check = [\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"]\n",
        "df_streaming.select(*[\n",
        "    (\n",
        "        F.count(F.when((F.isnan(c) | F.col(c).isNull()), c)) if t not in (\"timestamp\", \"date\")\n",
        "        else F.count(F.when(F.col(c).isNull(), c))\n",
        "    ).alias(c)\n",
        "    for c, t in df_streaming.dtypes if c in cols_check\n",
        "]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU6r5A55VzFV",
        "outputId": "e71be3e6-a28c-4093-d19b-deb5134f7482"
      },
      "id": "PU6r5A55VzFV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+---+-----+---------+\n",
            "|Date|Open|High|Low|Close|Adj Close|\n",
            "+----+----+----+---+-----+---------+\n",
            "|   0|   0|   0|  0|    0|        0|\n",
            "+----+----+----+---+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_streaming.registerTempTable(\"df_streaming\")\n",
        "spark.sql(\"select avg(Volume) from df_streaming\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vTuHSyYy5FL",
        "outputId": "909024c8-5fa4-4998-d4eb-7f7a0124e8f5"
      },
      "id": "9vTuHSyYy5FL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|        avg(Volume)|\n",
            "+-------------------+\n",
            "|2.894781755361398E7|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_streaming.registerTempTable(\"df_streaming\")\n",
        "spark.sql(\"select avg(High) from df_streaming\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du2CqyBL8n9Q",
        "outputId": "75ea8845-93dd-4150-adef-19eb7b6ff55a"
      },
      "id": "du2CqyBL8n9Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|         avg(High)|\n",
            "+------------------+\n",
            "|162.64645748371714|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_streaming.registerTempTable(\"df_streaming\")\n",
        "spark.sql(\"select avg(Low) from df_streaming\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSXtyYKF8rug",
        "outputId": "2fa7699c-dedd-4167-aa25-57104e93d1f4"
      },
      "id": "GSXtyYKF8rug",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|         avg(Low)|\n",
            "+-----------------+\n",
            "|159.4894678467037|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_streaming.registerTempTable(\"df_streaming\")\n",
        "spark.sql(\"SELECT Date,Close,(AVG(Close)OVER(ORDER BY Date ROWS BETWEEN 3 PRECEDING AND CURRENT ROW), 2)AS rolling_average FROM df_streaming\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0cjFlTxml-D",
        "outputId": "9ef6d6e4-b749-4928-fc6f-5794bed838da"
      },
      "id": "s0cjFlTxml-D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+--------------------+\n",
            "|               Date|    Close|     rolling_average|\n",
            "+-------------------+---------+--------------------+\n",
            "|2017-02-22 00:00:00|64.360001|      {64.360001, 2}|\n",
            "|2017-02-23 00:00:00|64.620003|      {64.490002, 2}|\n",
            "|2017-02-24 00:00:00|64.620003|{64.5333356666666...|\n",
            "|2017-02-27 00:00:00|64.230003|     {64.4575025, 2}|\n",
            "|2017-02-28 00:00:00|    63.98|    {64.36250225, 2}|\n",
            "|2017-03-01 00:00:00|64.940002|      {64.442502, 2}|\n",
            "|2017-03-02 00:00:00|64.010002|    {64.29000175, 2}|\n",
            "|2017-03-03 00:00:00|    64.25|      {64.295001, 2}|\n",
            "|2017-03-06 00:00:00|64.269997|    {64.36750025, 2}|\n",
            "|2017-03-07 00:00:00|64.400002|{64.2325002499999...|\n",
            "|2017-03-08 00:00:00|64.989998|    {64.47749925, 2}|\n",
            "|2017-03-09 00:00:00|64.730003|{64.5975000000000...|\n",
            "|2017-03-10 00:00:00|    64.93|    {64.76250075, 2}|\n",
            "|2017-03-13 00:00:00|64.709999|          {64.84, 2}|\n",
            "|2017-03-14 00:00:00|64.410004|     {64.6950015, 2}|\n",
            "|2017-03-15 00:00:00|    64.75|    {64.70000075, 2}|\n",
            "|2017-03-16 00:00:00|64.639999|     {64.6275005, 2}|\n",
            "|2017-03-17 00:00:00|64.870003|     {64.6675015, 2}|\n",
            "|2017-03-20 00:00:00|    64.93|     {64.7975005, 2}|\n",
            "|2017-03-21 00:00:00|64.209999|    {64.66250025, 2}|\n",
            "+-------------------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Output is stored in csv format\n",
        "df_streaming.write.csv(\"output_\")"
      ],
      "metadata": {
        "id": "88AFRH2HLFxm"
      },
      "id": "88AFRH2HLFxm",
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Aritecture of the pipeline:\n",
        "\n",
        "Approach One:Using the Spark technolgy in which receiving the dataset continously by kafa(stream.read)method is Used.then calculating\n",
        "basic query as per question2 ,average for low,high,volume columns and caculating the rolling_average to ecevute the spark sql query and store the output in CSV.\n",
        "\n",
        "Approach Two:Using the Dag capbility in which file input path is FILE_PATH_INPUT to store the CSV file in GCP and output path is FILE_PATH_OUTPUT to store the output then \n",
        "send the report to mapped the send_report mathod inside the Dag and calculate the flag_anomaly method to send the report\n",
        "after calculating avgerage rolling stock tp slack API which is mapped in dag code \n",
        "\n",
        "https://hooks.slack.com/services/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\n",
        "FILE_PATH_INPUT = '/home/airflow/gcs/data/input/'\n",
        "FILE_PATH_OUTPUT = '/home/airflow/gcs/data/output/' \n"
      ],
      "metadata": {
        "id": "R-d2nSqrqGuh"
      },
      "id": "R-d2nSqrqGuh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}